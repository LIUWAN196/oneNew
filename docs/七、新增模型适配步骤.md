### 7.1 模型转换
参考 9.2 节，将 onnx 转为 one 格式的模型，并且查看是否有 LOG ERR 报错，一般是查看是否有不支持的算子

### 7.2 用户新增自定义算子
假设需要新增的算子为 Foo  
**a、增加 config；增加 host 侧的类管理和算子注册；增加 dev 侧的 forward 实现**  
在 /oneNew/common/nn_common.h 中，参照 CONV_CONFIG_S 等结构体形式，为 Foo 算子增加一个 config 为 FOO_CONFIG_S  
**b、增加 ONNX 的 IR 向 config 转换的实现**  
在 oneNew/tools/onnx2one/onnx2one.cpp 中，搜索 “if (op_type == "Conv")” 代码，在附近位置，增加一个 "else if (op_type == "Foo")" 的转换。（如果 Foo 算子除了输入、输出 tensor 外，没有其余的属性，则可以不增加都可，因为 tensor 这类的公共属性，会被统一处理）  
**c、增加 host 侧的类管理和算子注册**   
参考 oneNew/host/ops/conv.h，在同级目录下，新建一个 foo.h 文件，并且在该文件中，完成 create_instance、shape_infer 等多个成员函数实现。同时，在最下方，通过 OP_REGISTER_GLOBAL 宏，将 Foo 算子注册到 Manager 单例中。  
**d、增加 dev 侧的 forward 实现**  
参考 oneNew/device/x86/Conv 文件夹，完成 Foo 算子的具体实现过程，这个就是读取 FOO_CONFIG_S 和输入输出 tensor 描述以及指针。直接计算即可。
### 7.3 适配新的模型
在完成 8 算子新增后，自动完成新模型的编译适配，后续就是需要去保证模型的正确性。  

### 7.4 调试模型正确性
参考 8.2 节，使用 model_exc_type: ofmap_dumping 模式。把 one new 推理的中间层输出都 dump 到指定位置，然后用 python 跑 onnxruntime，逐层对比数据的准确性，看哪个节点卡住或者相似度低。  
然后再去看该节点是 shape infer 出了问题，还是属性的描述不符合预期，或者内部计算有问题。来逐步调试该层的推理正确性。

### 7.5 性能调优
参考 9.4 节，结果正确后，配置 model_exc_type: perf_profiling 使用性能模式，dump 出每个 layer 的性能数据，看哪类 op 或者哪个 layer 利用率较低，进行优化
参考 9.3 节，如果有需要，通过 optimize 来量化模型或者做算子融合。  

### 7.6 添加后处理
性能优化结束后，为模型增加有必要的后处理（例如分类，目标识别等）  
这部分参考 oneNew/example/Normal_model.h 的 87 ～ 96 行代码即可。在 oneNew/example/post_process.hpp 中，已经有分类、目标检测、姿态检测、分割这 4 种后处理实现，以及结果显示的实现了，如果有需要，可以在这里再增加其余的后处理实现。

### 7.7 将模型加入集成测试
参考 8.1 节将模型加入 ci 测试，保证在进行下一个模型适配工作的前后，能快速把已经适配好的模型快速跑一边 ci 测试。保证新适配的模型不会影响已经适配好的模型。  

### 7.8 完成新模型的适配工作
